import logging
import uuid
from types import SimpleNamespace

from core.celery_app import celery_app
from django.db import transaction
from messaging.constants import PersistChatEntry, TopicStabilityUpdated
from realtime.constants import INITIATION_REFINED_TOPIC
from realtime.utils import send_ws_notification

from .models import (ChatHistoryEntry, InitiationPhaseData,
                     ResearchWorkflow)
from .serializers import RefinedTopicSerializer
from .utils import determine_feasibility_status, get_resource_suggestion

logger = logging.getLogger(__name__)


@celery_app.task(name=TopicStabilityUpdated.name, ignore_result=True)
def update_topic_stability_data(event_type: str, payload: dict):
    """
    Consumer task for TOPIC_STABILITY_UPDATED event.

    Persists structured data generated by the Topic Refinement Agent (TR Agent)
    to the InitiationPhaseData model and related tables (TopicKeyword, TopicScopeElement).

    Args:
        payload: Contains session_id, new_stability_score, is_topic_too_niche,
                 current_research_question, refined_keywords_to_lock, refined_scope_to_lock,
                 last_chat_sequence_number, etc.
    """
    task_id = update_topic_stability_data.request.id
    session_id = payload.get('session_id')
    user_id = payload.get('user_id')

    if user_id is None:
        logger.error("Task %s: Missing user_id in payload. Aborting.", task_id)
        return

    try:
        # Fetch the InitiationPhaseData instance linked to the session
        # NOTE: This requires fetching ResearchWorkflow first to get the PK link
        initiation_data = InitiationPhaseData.objects.select_related(
            'workflow'
        ).get(
            workflow_id=session_id
        )
    except InitiationPhaseData.DoesNotExist:
        logger.error("Task %s: InitiationPhaseData not found for session %s. Aborting.", task_id, session_id)
        return

    logger.info("Task %s: Persisting TR Agent data for session %s. Score: %s",
                task_id, session_id, payload.get('new_stability_score'))

    new_stability_score = payload.get('new_stability_score')
    if not new_stability_score or new_stability_score is None:
        new_stability_score = initiation_data.stability_score

    current_research_question = payload.get('current_research_question')
    if not current_research_question or current_research_question is None:
        current_research_question = initiation_data.final_research_question

    is_niche = payload.get('is_topic_too_niche')
    if not is_niche or is_niche is None:
        is_niche = False

    updated_summary = payload.get('updated_summary')
    if not updated_summary or updated_summary is None:
        updated_summary = initiation_data.conversation_summary

    last_chat_sequence_number = payload.get('last_chat_sequence_number')
    if not last_chat_sequence_number or last_chat_sequence_number is None:
        last_chat_sequence_number = initiation_data.last_analysis_sequence_number

    initiation_data.stability_score = new_stability_score
    initiation_data.final_research_question = current_research_question

    # NOTE: Feasibility Status is calculated by the backend *after* the score and structural analysis.
    # We assume a separate function determines the final feasibility_status based on stability_score and is_topic_too_niche.
    initiation_data.feasibility_status = determine_feasibility_status(initiation_data.stability_score, is_niche)
    initiation_data.conversation_summary = updated_summary
    initiation_data.last_analysis_sequence_number = last_chat_sequence_number

    # initiation_data.agent_evaluation_count += 1 # Increment evaluation count
    initiation_data.save(update_fields=[
        'stability_score',
        'final_research_question',
        'feasibility_status',
        'conversation_summary',
        'last_analysis_sequence_number',
        'updated_at'])

    refined_keywords = payload.get('refined_keywords_to_lock', [])
    keywords = initiation_data.workflow.keywords.all()
    keyword_set = set([keyword.label for keyword in keywords])
    TopicKeyword = initiation_data.workflow.keywords.model
    new_keywords = []
    for keyword_label in refined_keywords:
        if keyword_label in keyword_set:
            continue

        new_keyword = TopicKeyword(
            label=keyword_label,
            status='AI_EXTRACTED'
        )

        # Create a concetual node in canvas
        ConceptualNode = new_keyword.node.model
        new_conceptual_node = ConceptualNode(
            label=keyword_label,
            node_type='CONCEPT',
        )
        new_keyword.node.add(new_conceptual_node, bulk=False)

        new_keywords.append(new_keyword)

    if new_keywords:
        initiation_data.workflow.keywords.add(*new_keywords, bulk=False)

    scope_elements = initiation_data.workflow.scope_elements.all()
    scope_element_set = set([(scope_element.label, scope_element.rationale) for scope_element in scope_elements])
    TopicScopeElement = initiation_data.workflow.scope_elements.model
    new_scope_elements = []
    refined_scope_elements = payload.get('refined_scope_to_lock', [])
    for element in refined_scope_elements:
        scope_label = element.get('label')
        scope_rationale=element.get('rationale')
        if (scope_label, scope_rationale) in scope_element_set:
            continue

        new_scope = TopicScopeElement(
            label=scope_label,
            rationale=scope_rationale,
            status='AI_EXTRACTED'
        )

        # Create a concetual node in canvas
        ConceptualNode = new_scope.node.model
        new_conceptual_node = ConceptualNode(
            label=scope_label,
            node_type='GROUP',
        )
        new_scope.node.add(new_conceptual_node, bulk=False)

        new_scope_elements.append(new_scope)

    if new_scope_elements:
        initiation_data.workflow.scope_elements.add(*new_scope_elements, bulk=False)

    refined_topic = SimpleNamespace(
            stability_score=initiation_data.stability_score,
            feasibility_status=initiation_data.feasibility_status,
            final_research_question=initiation_data.final_research_question,
            keywords=initiation_data.workflow.keywords.all(),
            scope_elements=initiation_data.workflow.scope_elements.all(),
            resource_suggestion=get_resource_suggestion(initiation_data.feasibility_status)
    )

    refined_topic_payload = {
        "message": "Topic refinement data updated.",
    }
    refined_topic_payload.update(RefinedTopicSerializer(refined_topic).data)
    send_ws_notification(
        user_id=user_id,
        event_type=INITIATION_REFINED_TOPIC,
        payload=refined_topic_payload
    )

    logger.info("Task %s: Database update complete for session %s.", task_id, session_id)


@celery_app.task(name=PersistChatEntry.name, ignore_result=True)
def persist_chat_entry(event_type: str, payload: dict):
    """
    Celery task to asynchronously persist a single chat message entry
    to the ChatHistoryEntry database table.

    Args:
        session_id: The UUID of the ResearchWorkflow to link the message to.
        role: The sender's role ('user' or 'system').
        content: The text content of the message.
        name: The specific sender name (e.g., 'Explorer Agent').
        sequence_number: The sequential order of the message within the workflow.

    Returns:
        bool: True if the entry was successfully persisted, False otherwise.
    """
    task = persist_chat_entry
    session_id = payload.get('session_id')
    role = payload.get('role')
    content = payload.get('content')
    name = payload.get('name')
    sequence_number = payload.get('sequence_number')

    try:
        # Look up the ResearchWorkflow instance
        # Retrieve the workflow state using the provided session_id UUID.
        workflow = ResearchWorkflow.objects.get(session_id=uuid.UUID(session_id))
    except ResearchWorkflow.DoesNotExist:
        # If the workflow state is not found, log an error and stop the task without retrying.
        logger.error(f"EntityStatus with ID {session_id} not found. Aborting chat persistence.")
        return False
    except Exception as e:
        # Handle other retrieval errors and initiate a retry mechanism.
        logger.warning(f"Error fetching EntityStatus {session_id} before persisting chat: {e}. Retrying in 60s.")
        # Retry the task after 60 seconds
        raise task.retry(exc=e, countdown=60)

    try:
        # Create and save the ChatHistoryEntry instance within an atomic transaction
        with transaction.atomic():
            ChatHistoryEntry.objects.create(
                workflow=workflow,
                role=role,
                content=content,
                name=name,
                sequence_number=sequence_number,
                # 'id' and 'timestamp' are auto-generated by the model
            )

        logger.info(f"Successfully persisted chat entry for session {session_id}, sequence {sequence_number}.")
        return True

    except Exception as e:
        # Handle persistence errors (e.g., database connection issues) and retry
        logger.error(f"Database error while persisting chat entry for session {session_id}: {e}. Retrying in 120s.")
        # Retry the task after 120 seconds.
        raise task.retry(exc=e, countdown=120)